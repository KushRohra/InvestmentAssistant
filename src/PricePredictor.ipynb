{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a51344e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6690af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp\tOpen\tHigh\tLow\tClose\tVolume_(BTC)\tVolume_(Currency)\tWeighted_Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6c14f",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15ac20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.functions import dayofweek, hour\n",
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae399f7",
   "metadata": {},
   "source": [
    "# Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3616872",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"BitcoinDataProcessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166af6ad",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b641b2c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o58.load.\n: java.lang.ClassNotFoundException: ../jars/postgresql-42.7.1.jar\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dedbe505e7e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# db_properties[\"driver\"] = \"org.postgresql.Driver\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtable_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"crypto_price_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbitcoin_data_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jdbc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     def json(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o58.load.\n: java.lang.ClassNotFoundException: ../jars/postgresql-42.7.1.jar\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "db_url = \"jdbc:postgresql://localhost:5432/mydatabase\"\n",
    "db_properties = {\n",
    "    \"user\": \"myusername\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"driver\": \"../jars/postgresql-42.7.1.jar\"\n",
    "}\n",
    "# jdbc_driver_path = \"../jars/postgresql-42.7.1.jar\"\n",
    "# spark.sparkContext.addFile(jdbc_driver_path)\n",
    "# db_properties[\"driver\"] = \"org.postgresql.Driver\"\n",
    "table_name = \"crypto_price_data\"\n",
    "bitcoin_data_full = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", db_properties['user']) \\\n",
    "    .option(\"password\", db_properties['password']) \\\n",
    "    .option(\"driver\", db_properties['driver']) \\\n",
    "    .load()\n",
    "# bitcoin_data_full = spark.read.csv(\"./bitcoin_historic_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0caa2e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_data = bitcoin_data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f3de5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Close    1243608\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_data_full.toPandas()[['Close']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90cf01c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4857377"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_data_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee64fb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bitcoin_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57841ed8",
   "metadata": {},
   "source": [
    "# Data Cleaning - Time stamp correction, Missing value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2798f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the timestamp from unixtimestamp to a readable format\n",
    "bitcoin_data = bitcoin_data.withColumn(\"Timestamp\", bitcoin_data[\"Timestamp\"].cast(\"timestamp\"))\n",
    "\n",
    "# Extracting the date part from the timestamp\n",
    "bitcoin_data = bitcoin_data.withColumn(\"Date\", F.to_date(\"Timestamp\"))\n",
    "\n",
    "# Define a window specification partitioned by date and ordered by timestamp\n",
    "window_spec = Window.partitionBy(\"Date\").orderBy(\"Timestamp\")\n",
    "\n",
    "# Getting the first record of each day\n",
    "bitcoin_data = bitcoin_data.withColumn(\"row_num\", F.row_number().over(window_spec)).filter(F.col(\"row_num\") == 1).drop(\"row_num\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fdb7db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Close    675\n",
       " dtype: int64,\n",
       " Close    2700\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_data.toPandas()[['Close']].isnull().sum(), bitcoin_data.toPandas()[['Close']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6a8ca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+----------+\n",
      "|          Timestamp|Open|High| Low|Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|      Date|\n",
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+----------+\n",
      "|2011-12-31 13:22:00|4.39|4.39|4.39| 4.39|  0.45558087|     2.0000000193|          4.39|2011-12-31|\n",
      "|2012-01-01 00:00:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2012-01-01|\n",
      "|2012-01-02 00:00:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2012-01-02|\n",
      "|2012-01-03 00:00:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2012-01-03|\n",
      "|2012-01-04 00:00:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2012-01-04|\n",
      "|2012-01-05 00:00:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2012-01-05|\n",
      "|2012-01-06 00:00:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2012-01-06|\n",
      "|2012-01-07 00:00:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2012-01-07|\n",
      "|2012-01-08 00:00:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2012-01-08|\n",
      "|2012-01-09 00:00:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2012-01-09|\n",
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bitcoin_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89845f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3375"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ede3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, last\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "other_columns = [\"Weighted_Price\", \"Volume_(BTC)\", \"Volume_(Currency)\"]\n",
    "for col_name in other_columns:\n",
    "#     bitcoin_data = bitcoin_data.withColumn(col_name, when(col(col_name).isNull(), 0).otherwise(col(col_name)))\n",
    "    bitcoin_data = bitcoin_data.na.fill(value=0,subset=[col_name])\n",
    "\n",
    "# Fill forward for OHLC data\n",
    "# ohlc_columns = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "# for col_name in ohlc_columns:\n",
    "#     # Create a window specification to fill forward based on timestamp\n",
    "#     window_spec = Window.orderBy(\"Timestamp\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "#     # Fill forward using the last non-null value\n",
    "#     bitcoin_data = bitcoin_data.withColumn(col_name, last(col_name, ignorenulls=True).over(window_spec))\n",
    "\n",
    "# Fill forward for OHLC data    \n",
    "ohlc_columns = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "bitcoin_data_pd = bitcoin_data.toPandas() # Converting to pandas df since Spark function not working as expected \n",
    "for col_name in ohlc_columns:\n",
    "    bitcoin_data_pd[col_name].fillna(method='ffill', inplace=True)\n",
    "\n",
    "bitcoin_data = spark.createDataFrame(bitcoin_data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "900986d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3375, 9)\n",
      "Columns: ['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Weighted_Price', 'Date']\n",
      "Is There any 'NaN' value: False\n",
      "Is there any duplicate value: False\n"
     ]
    }
   ],
   "source": [
    "# Shape (Number of Rows, Number of Columns)\n",
    "num_rows = bitcoin_data.count()\n",
    "num_columns = len(bitcoin_data.columns)\n",
    "\n",
    "print(\"Shape: ({}, {})\".format(num_rows, num_columns))\n",
    "\n",
    "# Columns\n",
    "columns = bitcoin_data.columns\n",
    "print(\"Columns: {}\".format(columns))\n",
    "\n",
    "# Check for 'NaN' values\n",
    "has_nan = bitcoin_data.select([col(c).isNull().alias(c) for c in bitcoin_data.columns]).rdd.flatMap(lambda x: x).collect()\n",
    "any_nan = any(has_nan)\n",
    "\n",
    "print(\"Is There any 'NaN' value: {}\".format(any_nan))\n",
    "\n",
    "# Check for duplicate values\n",
    "has_duplicates = bitcoin_data.groupBy(bitcoin_data.columns).count().filter(\"count > 1\").count() > 0\n",
    "\n",
    "print(\"Is there any duplicate value: {}\".format(has_duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3035f",
   "metadata": {},
   "source": [
    "# Feature Engineering - Day of Week, 7 day prior information features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d56e383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Data Shape:  3368\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# You can add features like day of the week, hour of the day, etc.\n",
    "bitcoin_data = bitcoin_data.withColumn(\"DayOfWeek\", dayofweek(\"Timestamp\"))\n",
    "bitcoin_data = bitcoin_data.withColumn(\"HourOfDay\", hour(\"Timestamp\"))\n",
    "\n",
    "# Getting more features, last 7 days information to help predict next day closing (can increase more)\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window().orderBy(\"Timestamp\")\n",
    "\n",
    "lag_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume_(BTC)\", \"Volume_(Currency)\"]\n",
    "\n",
    "# Create lagged features for the last 7 days\n",
    "for i in lag_cols:\n",
    "    for j in range(1,8):\n",
    "        bitcoin_data = bitcoin_data.withColumn(col_name + \"_b_\" + str(j), lag(col_name, j).over(window_spec))\n",
    "\n",
    "bitcoin_data = bitcoin_data.dropna() # drop the first rows. They don't have previous information \n",
    "print(\"Updated Data Shape: \", bitcoin_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "119ac24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+----------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|          Timestamp|Open|High| Low|Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|      Date|DayOfWeek|HourOfDay|Close_b_1|Close_b_2|Close_b_3|Close_b_4|Close_b_5|Close_b_6|Close_b_7|\n",
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+----------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|2012-01-07 00:00:00|4.39|4.39|4.39| 4.39|         0.0|              0.0|           0.0|2012-01-07|        7|        0|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|\n",
      "|2012-01-08 00:00:00|4.39|4.39|4.39| 4.39|         0.0|              0.0|           0.0|2012-01-08|        1|        0|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|\n",
      "|2012-01-09 00:00:00|4.39|4.39|4.39| 4.39|         0.0|              0.0|           0.0|2012-01-09|        2|        0|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|\n",
      "|2012-01-10 00:00:00|4.39|4.39|4.39| 4.39|         0.0|              0.0|           0.0|2012-01-10|        3|        0|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|\n",
      "|2012-01-11 00:00:00|4.39|4.39|4.39| 4.39|         0.0|              0.0|           0.0|2012-01-11|        4|        0|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|\n",
      "|2012-01-12 00:00:00|4.39|4.39|4.39| 4.39|         0.0|              0.0|           0.0|2012-01-12|        5|        0|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|\n",
      "|2012-01-13 00:00:00|4.39|4.39|4.39| 4.39|         0.0|              0.0|           0.0|2012-01-13|        6|        0|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|\n",
      "|2012-01-14 00:00:00|4.39|4.39|4.39| 4.39|         0.0|              0.0|           0.0|2012-01-14|        7|        0|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|\n",
      "|2012-01-15 00:00:00|4.39|4.39|4.39| 4.39|         0.0|              0.0|           0.0|2012-01-15|        1|        0|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|\n",
      "|2012-01-16 00:00:00|4.39|4.39|4.39| 4.39|         0.0|              0.0|           0.0|2012-01-16|        2|        0|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|     4.39|\n",
      "+-------------------+----+----+----+-----+------------+-----------------+--------------+----------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bitcoin_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7488b9d",
   "metadata": {},
   "source": [
    "# Generate the Prediction label as next day close value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61c35d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated record count after adding prediction close label: 3367\n"
     ]
    }
   ],
   "source": [
    "# Add next closing value as a label for prediction label\n",
    "window_spec = Window().orderBy(\"Timestamp\")\n",
    "\n",
    "bitcoin_data = bitcoin_data.withColumn(\"upcoming_close\", lag('Close', -1).over(window_spec))\n",
    "\n",
    "bitcoin_data = bitcoin_data.dropna() # drop the last row. It doesn't have next information \n",
    "print(\"Updated record count after adding prediction close label:\", bitcoin_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad32c963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp',\n",
       " 'Open',\n",
       " 'High',\n",
       " 'Low',\n",
       " 'Close',\n",
       " 'Volume_(BTC)',\n",
       " 'Volume_(Currency)',\n",
       " 'Weighted_Price',\n",
       " 'Date',\n",
       " 'DayOfWeek',\n",
       " 'HourOfDay',\n",
       " 'Close_b_1',\n",
       " 'Close_b_2',\n",
       " 'Close_b_3',\n",
       " 'Close_b_4',\n",
       " 'Close_b_5',\n",
       " 'Close_b_6',\n",
       " 'Close_b_7',\n",
       " 'upcoming_close']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479fc428",
   "metadata": {},
   "source": [
    "# Generate the Feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "114a3d41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|          Attributes|upcoming_close|\n",
      "+--------------------+--------------+\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          4.39|\n",
      "|[4.39,4.39,4.39,4...|          6.01|\n",
      "|[6.01,6.01,6.01,6...|          6.01|\n",
      "|[6.01,6.01,6.01,6...|          6.01|\n",
      "|[6.01,6.01,6.01,6...|          6.01|\n",
      "|[6.01,6.01,6.01,6...|          6.01|\n",
      "|[6.01,6.01,6.01,6...|          6.01|\n",
      "|[6.01,6.01,6.01,6...|           6.0|\n",
      "|[6.09,6.09,6.0,6....|           6.0|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Input all the features in one vector column\n",
    "feature_columns = [i for i in bitcoin_data.columns if i not in ['upcoming_close', 'Date', 'Timestamp']]\n",
    "assembler = VectorAssembler(inputCols= feature_columns, outputCol = 'Attributes')\n",
    "\n",
    "output = assembler.transform(bitcoin_data) #transform the data using Vector Assembler\n",
    "\n",
    "#Input vs Output\n",
    "finalized_data = output.select(\"Attributes\",\"upcoming_close\")\n",
    "\n",
    "finalized_data.show() # showing the final data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6394a8d5",
   "metadata": {},
   "source": [
    "# Feature Vector and Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e6d8549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% test data = % 10.008910008910009\n",
      "Train data records: 3030\n",
      "Test data records: 337\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection\n",
    "feature_columns = [i for i in bitcoin_data.columns if i not in ['upcoming_close', 'Date', 'Timestamp', 'HourOfDay']]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "# bitcoin_data = assembler.transform(bitcoin_data)\n",
    "\n",
    "# Train-Test Split - since the problem is time series, we should perform the sequenctial split\n",
    "prediction_days = int(np.round(bitcoin_data.count()*(10/100),0))\n",
    "train_data = bitcoin_data.limit(int(bitcoin_data.count()-prediction_days))\n",
    "test_data = bitcoin_data.exceptAll(train_data)\n",
    "\n",
    "print(\"% test data = %\", (prediction_days/int(bitcoin_data.count())) * 100)\n",
    "print(\"Train data records:\", train_data.count())\n",
    "print(\"Test data records:\", test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336622ba",
   "metadata": {},
   "source": [
    "# Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5672dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"upcoming_close\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# Model Training\n",
    "model_lr = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56b60d",
   "metadata": {},
   "source": [
    "# Model Evaluation using RMSE and R2 - LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "32fef2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 1260.6665093920308\n",
      "R2: 0.9934691260305369\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model_lr.transform(test_data)\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"upcoming_close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"upcoming_close\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(f\"R2: {r2}\")\n",
    "\n",
    "# # View Predictions\n",
    "# predictions.select(\"Close\", \"prediction\", *feature_columns).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe0b6d",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f5b4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"upcoming_close\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Model Training\n",
    "model_rf = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fc8417",
   "metadata": {},
   "source": [
    "# Model Evaluation using RMSE and R2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "563129a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 16995.15992452752\n",
      "R2: -0.1869198227710125\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model_rf.transform(test_data)\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"upcoming_close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"upcoming_close\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(f\"R2: {r2}\")\n",
    "\n",
    "# # View Predictions\n",
    "# predictions.select(\"Close\", \"prediction\", *feature_columns).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab6427",
   "metadata": {},
   "source": [
    "# Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "221eb526",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"upcoming_close\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "# Model Training\n",
    "model_gbt = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552195a",
   "metadata": {},
   "source": [
    "# Model Evaluation using RMSE and R2 - GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52fa1dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 16015.088058957046\n",
      "R2: -0.05397311689097095\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model_gbt.transform(test_data)\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"upcoming_close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"upcoming_close\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "print(f\"R2: {r2}\")\n",
    "\n",
    "# # View Predictions\n",
    "# predictions.select(\"Close\", \"prediction\", *feature_columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed83ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
